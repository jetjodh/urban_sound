{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " #!pip install librosa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import librosa as lib\\nimport os\\nimport pandas as pd\\nimport pylab\\nimport numpy as np\\nimport librosa.display\\nimport glob '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import librosa as lib\n",
    "import os\n",
    "import pandas as pd\n",
    "import pylab\n",
    "import numpy as np\n",
    "import librosa.display\n",
    "import glob \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def convert(filename):   \\n    sig, fs = lib.load(filename)   \\n    # make pictures name \\n    save_path = filename+'.jpg'\\n\\n    pylab.axis('off') # no axis\\n    pylab.axes([0., 0., 1., 1.], frameon=False, xticks=[], yticks=[]) # Remove the white edge\\n    S = librosa.feature.melspectrogram(y=sig, sr=fs)\\n    lib.display.specshow(librosa.power_to_db(S, ref=np.max))\\n    pylab.savefig(save_path, bbox_inches=None, pad_inches=0)\\n    pylab.close()\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def convert(filename):   \n",
    "    sig, fs = lib.load(filename)   \n",
    "    # make pictures name \n",
    "    save_path = filename+'.jpg'\n",
    "\n",
    "    pylab.axis('off') # no axis\n",
    "    pylab.axes([0., 0., 1., 1.], frameon=False, xticks=[], yticks=[]) # Remove the white edge\n",
    "    S = librosa.feature.melspectrogram(y=sig, sr=fs)\n",
    "    lib.display.specshow(librosa.power_to_db(S, ref=np.max))\n",
    "    pylab.savefig(save_path, bbox_inches=None, pad_inches=0)\n",
    "    pylab.close()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\"\"\"for i in range(11):\n",
    "#    for filename in glob.glob(r'D:\\Projects\\urban_sound\\UrbanSound8K\\audio\\fold'+str(i)+'\\*.wav'):\n",
    " #       convert(filename)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(r'D:\\Projects\\urban_sound\\UrbanSound8K\\metadata\\UrbanSound8K.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      slice_file_name    fsID  start        end  salience  fold  classID  \\\n",
      "0    100032-3-0-0.wav  100032    0.0   0.317551         1     5        3   \n",
      "1  100263-2-0-117.wav  100263   58.5  62.500000         1     5        2   \n",
      "2  100263-2-0-121.wav  100263   60.5  64.500000         1     5        2   \n",
      "3  100263-2-0-126.wav  100263   63.0  67.000000         1     5        2   \n",
      "4  100263-2-0-137.wav  100263   68.5  72.500000         1     5        2   \n",
      "\n",
      "              class  \n",
      "0          dog_bark  \n",
      "1  children_playing  \n",
      "2  children_playing  \n",
      "3  children_playing  \n",
      "4  children_playing  \n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data.loc[data['fold'] == 8]\n",
    "train = data.loc[data['fold'] != 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv2.imshow('in',cv2.imread(r'D:\\Projects\\urban_sound\\UrbanSound8K\\audio\\fold7\\99812-1-6-0.wav.jpg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7926,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "loc = r'D:\\Projects\\urban_sound\\UrbanSound8K\\audio'\n",
    "img_lst = np.array(train.slice_file_name.tolist())\n",
    "folds = np.array(train.fold)\n",
    "print(img_lst.shape)\n",
    "img_lst = [loc +'\\\\'+'fold'+str(fold)+'\\\\'+ s + '.jpg' for s,fold in zip(img_lst,folds)]\n",
    "X = []\n",
    "for s in img_lst:\n",
    "    X.append(cv2.resize(cv2.imread(s, cv2.IMREAD_COLOR),(150,150),interpolation=cv2.INTER_CUBIC))\n",
    "#X = [cv2.imshow('img',s) for s in X]\n",
    "\n",
    "y = np.array(train.classID.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 2 2 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "#print(X[0])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X /= 255.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7926, 150, 150, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# construct the image generator for data augmentation\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "aug = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "lb = LabelEncoder()\n",
    "y = np_utils.to_categorical(lb.fit_transform(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "#base = keras.applications.nasnet.NASNetLarge(input_shape=None, include_top=False, weights=None, input_tensor=None, pooling=None, classes=10)\n",
    "base = keras.applications.inception_resnet_v2.InceptionResNetV2(include_top=False, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(806, 150, 150, 3)\n"
     ]
    }
   ],
   "source": [
    "test_lst = np.array(test.slice_file_name.tolist())\n",
    "folds = np.array(test.fold)\n",
    "test_lst = [loc +'\\\\'+'fold'+str(fold)+'\\\\'+ s + '.jpg' for s,fold in zip(test_lst,folds)]\n",
    "#print(test_lst)\n",
    "test_X = []\n",
    "for s in test_lst:\n",
    "    test_X.append(cv2.resize(cv2.imread(s, cv2.IMREAD_COLOR),(150,150),interpolation=cv2.INTER_CUBIC))\n",
    "#X = [cv2.imshow('img',s) for s in X]\n",
    "test_X = np.array(test_X)\n",
    "test_X = test_X.astype('float')\n",
    "test_X /= 255.0 \n",
    "test_y = np.array(test.classID.tolist())\n",
    "test_y = np_utils.to_categorical(lb.fit_transform(test_y))\n",
    "print(test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "164/164 [==============================] - 184s 1s/step - loss: 0.7554 - categorical_accuracy: 0.7445 - top_k_categorical_accuracy: 0.9630\n",
      "Epoch 2/25\n",
      "164/164 [==============================] - 131s 796ms/step - loss: 0.2225 - categorical_accuracy: 0.9300 - top_k_categorical_accuracy: 0.9985\n",
      "Epoch 3/25\n",
      "164/164 [==============================] - 138s 839ms/step - loss: 0.1084 - categorical_accuracy: 0.9638 - top_k_categorical_accuracy: 0.9994\n",
      "Epoch 4/25\n",
      "164/164 [==============================] - 130s 793ms/step - loss: 0.0650 - categorical_accuracy: 0.9798 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 5/25\n",
      "164/164 [==============================] - 130s 793ms/step - loss: 0.0423 - categorical_accuracy: 0.9869 - top_k_categorical_accuracy: 0.9999\n",
      "Epoch 6/25\n",
      "164/164 [==============================] - 130s 794ms/step - loss: 0.0320 - categorical_accuracy: 0.9909 - top_k_categorical_accuracy: 0.9999\n",
      "Epoch 7/25\n",
      "164/164 [==============================] - 130s 793ms/step - loss: 0.0596 - categorical_accuracy: 0.9813 - top_k_categorical_accuracy: 0.9996\n",
      "Epoch 8/25\n",
      "164/164 [==============================] - 130s 793ms/step - loss: 0.0357 - categorical_accuracy: 0.9873 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 9/25\n",
      "164/164 [==============================] - 130s 794ms/step - loss: 0.0174 - categorical_accuracy: 0.9945 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 10/25\n",
      "164/164 [==============================] - 130s 794ms/step - loss: 0.0427 - categorical_accuracy: 0.9867 - top_k_categorical_accuracy: 0.9997\n",
      "Epoch 11/25\n",
      "164/164 [==============================] - 130s 795ms/step - loss: 0.0314 - categorical_accuracy: 0.9911 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 12/25\n",
      "164/164 [==============================] - 130s 794ms/step - loss: 0.0142 - categorical_accuracy: 0.9961 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 13/25\n",
      "164/164 [==============================] - 130s 794ms/step - loss: 0.0449 - categorical_accuracy: 0.9863 - top_k_categorical_accuracy: 0.9999\n",
      "Epoch 14/25\n",
      "164/164 [==============================] - 130s 794ms/step - loss: 0.0139 - categorical_accuracy: 0.9964 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 15/25\n",
      "164/164 [==============================] - 130s 794ms/step - loss: 0.0058 - categorical_accuracy: 0.9981 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 16/25\n",
      "164/164 [==============================] - 130s 794ms/step - loss: 0.0074 - categorical_accuracy: 0.9986 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 17/25\n",
      "164/164 [==============================] - 130s 794ms/step - loss: 0.0308 - categorical_accuracy: 0.9897 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 18/25\n",
      "164/164 [==============================] - 130s 794ms/step - loss: 0.0113 - categorical_accuracy: 0.9970 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 19/25\n",
      "164/164 [==============================] - 130s 793ms/step - loss: 0.0146 - categorical_accuracy: 0.9952 - top_k_categorical_accuracy: 0.9999\n",
      "Epoch 20/25\n",
      "164/164 [==============================] - 130s 794ms/step - loss: 0.0037 - categorical_accuracy: 0.9989 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 21/25\n",
      "164/164 [==============================] - 130s 794ms/step - loss: 0.0172 - categorical_accuracy: 0.9940 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 22/25\n",
      "164/164 [==============================] - 130s 794ms/step - loss: 0.0045 - categorical_accuracy: 0.9986 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 23/25\n",
      "164/164 [==============================] - 130s 794ms/step - loss: 0.0024 - categorical_accuracy: 0.9995 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 24/25\n",
      "164/164 [==============================] - 130s 794ms/step - loss: 0.0020 - categorical_accuracy: 0.9994 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 25/25\n",
      "164/164 [==============================] - 130s 794ms/step - loss: 0.0306 - categorical_accuracy: 0.9916 - top_k_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2179f20acf8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "BS = 48\n",
    "EPOCHS = 25\n",
    "x = base.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "model = Model(inputs=base.input, outputs=predictions)\n",
    "for layer in base.layers:\n",
    "    layer.trainable = True\n",
    "model.compile(optimizer=SGD(lr=0.01, momentum=0.9), loss='categorical_crossentropy', metrics=['categorical_accuracy','top_k_categorical_accuracy'])\n",
    "model.fit_generator(aug.flow(X, y, batch_size=BS),\n",
    "\tsteps_per_epoch=7895 // BS,\n",
    "\tepochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(r'D:\\Projects\\urban_sound\\model\\model_8.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "806/806 [==============================] - 15s 19ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1555418239388773, 0.7382133996516245, 0.9826302729528535]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=test_X, y=test_y, batch_size=None, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
