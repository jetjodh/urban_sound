{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " #!pip install librosa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import librosa as lib\\nimport os\\nimport pandas as pd\\nimport pylab\\nimport numpy as np\\nimport librosa.display\\nimport glob '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import librosa as lib\n",
    "import os\n",
    "import pandas as pd\n",
    "import pylab\n",
    "import numpy as np\n",
    "import librosa.display\n",
    "import glob \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def convert(filename):   \\n    sig, fs = lib.load(filename)   \\n    # make pictures name \\n    save_path = filename+'.jpg'\\n\\n    pylab.axis('off') # no axis\\n    pylab.axes([0., 0., 1., 1.], frameon=False, xticks=[], yticks=[]) # Remove the white edge\\n    S = librosa.feature.melspectrogram(y=sig, sr=fs)\\n    lib.display.specshow(librosa.power_to_db(S, ref=np.max))\\n    pylab.savefig(save_path, bbox_inches=None, pad_inches=0)\\n    pylab.close()\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def convert(filename):   \n",
    "    sig, fs = lib.load(filename)   \n",
    "    # make pictures name \n",
    "    save_path = filename+'.jpg'\n",
    "\n",
    "    pylab.axis('off') # no axis\n",
    "    pylab.axes([0., 0., 1., 1.], frameon=False, xticks=[], yticks=[]) # Remove the white edge\n",
    "    S = librosa.feature.melspectrogram(y=sig, sr=fs)\n",
    "    lib.display.specshow(librosa.power_to_db(S, ref=np.max))\n",
    "    pylab.savefig(save_path, bbox_inches=None, pad_inches=0)\n",
    "    pylab.close()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\"\"\"for i in range(11):\n",
    "#    for filename in glob.glob(r'D:\\Projects\\urban_sound\\UrbanSound8K\\audio\\fold'+str(i)+'\\*.wav'):\n",
    " #       convert(filename)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(r'D:\\Projects\\urban_sound\\UrbanSound8K\\metadata\\UrbanSound8K.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      slice_file_name    fsID  start        end  salience  fold  classID  \\\n",
      "0    100032-3-0-0.wav  100032    0.0   0.317551         1     5        3   \n",
      "1  100263-2-0-117.wav  100263   58.5  62.500000         1     5        2   \n",
      "2  100263-2-0-121.wav  100263   60.5  64.500000         1     5        2   \n",
      "3  100263-2-0-126.wav  100263   63.0  67.000000         1     5        2   \n",
      "4  100263-2-0-137.wav  100263   68.5  72.500000         1     5        2   \n",
      "\n",
      "              class  \n",
      "0          dog_bark  \n",
      "1  children_playing  \n",
      "2  children_playing  \n",
      "3  children_playing  \n",
      "4  children_playing  \n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data.loc[data['fold'] == 9]\n",
    "train = data.loc[data['fold'] != 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv2.imshow('in',cv2.imread(r'D:\\Projects\\urban_sound\\UrbanSound8K\\audio\\fold7\\99812-1-6-0.wav.jpg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7916,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "loc = r'D:\\Projects\\urban_sound\\UrbanSound8K\\audio'\n",
    "img_lst = np.array(train.slice_file_name.tolist())\n",
    "folds = np.array(train.fold)\n",
    "print(img_lst.shape)\n",
    "img_lst = [loc +'\\\\'+'fold'+str(fold)+'\\\\'+ s + '.jpg' for s,fold in zip(img_lst,folds)]\n",
    "X = []\n",
    "for s in img_lst:\n",
    "    X.append(cv2.resize(cv2.imread(s, cv2.IMREAD_COLOR),(150,150),interpolation=cv2.INTER_CUBIC))\n",
    "#X = [cv2.imshow('img',s) for s in X]\n",
    "\n",
    "y = np.array(train.classID.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 2 2 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "#print(X[0])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X /= 255.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7916, 150, 150, 3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the image generator for data augmentation\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "aug = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "lb = LabelEncoder()\n",
    "y = np_utils.to_categorical(lb.fit_transform(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "#base = keras.applications.nasnet.NASNetLarge(input_shape=None, include_top=False, weights=None, input_tensor=None, pooling=None, classes=10)\n",
    "base = keras.applications.inception_resnet_v2.InceptionResNetV2(include_top=False, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(816, 150, 150, 3)\n"
     ]
    }
   ],
   "source": [
    "test_lst = np.array(test.slice_file_name.tolist())\n",
    "folds = np.array(test.fold)\n",
    "test_lst = [loc +'\\\\'+'fold'+str(fold)+'\\\\'+ s + '.jpg' for s,fold in zip(test_lst,folds)]\n",
    "#print(test_lst)\n",
    "test_X = []\n",
    "for s in test_lst:\n",
    "    test_X.append(cv2.resize(cv2.imread(s, cv2.IMREAD_COLOR),(150,150),interpolation=cv2.INTER_CUBIC))\n",
    "#X = [cv2.imshow('img',s) for s in X]\n",
    "test_X = np.array(test_X)\n",
    "test_X = test_X.astype('float')\n",
    "test_X /= 255.0 \n",
    "test_y = np.array(test.classID.tolist())\n",
    "test_y = np_utils.to_categorical(lb.fit_transform(test_y))\n",
    "print(test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "328/328 [==============================] - 224s 682ms/step - loss: 0.7932 - categorical_accuracy: 0.7329 - top_k_categorical_accuracy: 0.9668\n",
      "Epoch 2/25\n",
      "328/328 [==============================] - 161s 491ms/step - loss: 0.2994 - categorical_accuracy: 0.9002 - top_k_categorical_accuracy: 0.9973\n",
      "Epoch 3/25\n",
      "328/328 [==============================] - 159s 486ms/step - loss: 0.1597 - categorical_accuracy: 0.9455 - top_k_categorical_accuracy: 0.9991\n",
      "Epoch 4/25\n",
      "328/328 [==============================] - 160s 487ms/step - loss: 0.1019 - categorical_accuracy: 0.9652 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 5/25\n",
      "328/328 [==============================] - 160s 488ms/step - loss: 0.0765 - categorical_accuracy: 0.9765 - top_k_categorical_accuracy: 0.9996\n",
      "Epoch 6/25\n",
      "328/328 [==============================] - 161s 490ms/step - loss: 0.0551 - categorical_accuracy: 0.9817 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 7/25\n",
      "328/328 [==============================] - 160s 489ms/step - loss: 0.0543 - categorical_accuracy: 0.9803 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 8/25\n",
      "328/328 [==============================] - 161s 490ms/step - loss: 0.0432 - categorical_accuracy: 0.9856 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 9/25\n",
      "328/328 [==============================] - 160s 488ms/step - loss: 0.0239 - categorical_accuracy: 0.9920 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 10/25\n",
      "328/328 [==============================] - 160s 489ms/step - loss: 0.0440 - categorical_accuracy: 0.9861 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 11/25\n",
      "328/328 [==============================] - 161s 490ms/step - loss: 0.0337 - categorical_accuracy: 0.9896 - top_k_categorical_accuracy: 0.9999\n",
      "Epoch 12/25\n",
      "328/328 [==============================] - 161s 490ms/step - loss: 0.0189 - categorical_accuracy: 0.9938 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 13/25\n",
      "328/328 [==============================] - 161s 490ms/step - loss: 0.0135 - categorical_accuracy: 0.9957 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 14/25\n",
      "328/328 [==============================] - 161s 490ms/step - loss: 0.0154 - categorical_accuracy: 0.9957 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 15/25\n",
      "328/328 [==============================] - 161s 490ms/step - loss: 0.0227 - categorical_accuracy: 0.9921 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 16/25\n",
      "328/328 [==============================] - 161s 491ms/step - loss: 0.0056 - categorical_accuracy: 0.9986 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 17/25\n",
      "328/328 [==============================] - 161s 490ms/step - loss: 0.0118 - categorical_accuracy: 0.9963 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 18/25\n",
      "328/328 [==============================] - 161s 491ms/step - loss: 0.0133 - categorical_accuracy: 0.9950 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 19/25\n",
      "328/328 [==============================] - 161s 490ms/step - loss: 0.0267 - categorical_accuracy: 0.9903 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 20/25\n",
      "328/328 [==============================] - 160s 489ms/step - loss: 0.0209 - categorical_accuracy: 0.9929 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 21/25\n",
      "328/328 [==============================] - 161s 489ms/step - loss: 0.0161 - categorical_accuracy: 0.9939 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 22/25\n",
      "328/328 [==============================] - 160s 489ms/step - loss: 0.0124 - categorical_accuracy: 0.9959 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 23/25\n",
      "328/328 [==============================] - 160s 489ms/step - loss: 0.0060 - categorical_accuracy: 0.9983 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 24/25\n",
      "328/328 [==============================] - 161s 490ms/step - loss: 0.0044 - categorical_accuracy: 0.9986 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 25/25\n",
      "328/328 [==============================] - 160s 489ms/step - loss: 0.0018 - categorical_accuracy: 0.9991 - top_k_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x259fb30e9b0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "BS = 24\n",
    "EPOCHS = 25\n",
    "x = base.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "model = Model(inputs=base.input, outputs=predictions)\n",
    "for layer in base.layers:\n",
    "    layer.trainable = True\n",
    "model.compile(optimizer=SGD(lr=0.01, momentum=0.9), loss='categorical_crossentropy', metrics=['categorical_accuracy','top_k_categorical_accuracy'])\n",
    "model.fit_generator(aug.flow(X, y, batch_size=BS),\n",
    "\tsteps_per_epoch=7895 // BS,\n",
    "\tepochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(r'D:\\Projects\\urban_sound\\model\\model.h5')\n",
    "model.save_weights(r'D:\\Projects\\urban_sound\\model\\model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "816/816 [==============================] - 5s 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2126487795456042, 0.8492647058823529, 0.9693627450980392]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=test_X, y=test_y, batch_size=None, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
