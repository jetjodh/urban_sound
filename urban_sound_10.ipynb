{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " #!pip install librosa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import librosa as lib\\nimport os\\nimport pandas as pd\\nimport pylab\\nimport numpy as np\\nimport librosa.display\\nimport glob '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import librosa as lib\n",
    "import os\n",
    "import pandas as pd\n",
    "import pylab\n",
    "import numpy as np\n",
    "import librosa.display\n",
    "import glob \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def convert(filename):   \\n    sig, fs = lib.load(filename)   \\n    # make pictures name \\n    save_path = filename+'.jpg'\\n\\n    pylab.axis('off') # no axis\\n    pylab.axes([0., 0., 1., 1.], frameon=False, xticks=[], yticks=[]) # Remove the white edge\\n    S = librosa.feature.melspectrogram(y=sig, sr=fs)\\n    lib.display.specshow(librosa.power_to_db(S, ref=np.max))\\n    pylab.savefig(save_path, bbox_inches=None, pad_inches=0)\\n    pylab.close()\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def convert(filename):   \n",
    "    sig, fs = lib.load(filename)   \n",
    "    # make pictures name \n",
    "    save_path = filename+'.jpg'\n",
    "\n",
    "    pylab.axis('off') # no axis\n",
    "    pylab.axes([0., 0., 1., 1.], frameon=False, xticks=[], yticks=[]) # Remove the white edge\n",
    "    S = librosa.feature.melspectrogram(y=sig, sr=fs)\n",
    "    lib.display.specshow(librosa.power_to_db(S, ref=np.max))\n",
    "    pylab.savefig(save_path, bbox_inches=None, pad_inches=0)\n",
    "    pylab.close()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\"\"\"for i in range(11):\n",
    "#    for filename in glob.glob(r'D:\\Projects\\urban_sound\\UrbanSound8K\\audio\\fold'+str(i)+'\\*.wav'):\n",
    " #       convert(filename)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(r'D:\\Projects\\urban_sound\\UrbanSound8K\\metadata\\UrbanSound8K.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      slice_file_name    fsID  start        end  salience  fold  classID  \\\n",
      "0    100032-3-0-0.wav  100032    0.0   0.317551         1     5        3   \n",
      "1  100263-2-0-117.wav  100263   58.5  62.500000         1     5        2   \n",
      "2  100263-2-0-121.wav  100263   60.5  64.500000         1     5        2   \n",
      "3  100263-2-0-126.wav  100263   63.0  67.000000         1     5        2   \n",
      "4  100263-2-0-137.wav  100263   68.5  72.500000         1     5        2   \n",
      "\n",
      "              class  \n",
      "0          dog_bark  \n",
      "1  children_playing  \n",
      "2  children_playing  \n",
      "3  children_playing  \n",
      "4  children_playing  \n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data.loc[data['fold'] == 10]\n",
    "train = data.loc[data['fold'] != 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7895,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "loc = r'D:\\Projects\\urban_sound\\UrbanSound8K\\audio'\n",
    "img_lst = np.array(train.slice_file_name.tolist())\n",
    "folds = np.array(train.fold)\n",
    "print(img_lst.shape)\n",
    "img_lst = [loc +'\\\\'+'fold'+str(fold)+'\\\\'+ s + '.jpg' for s,fold in zip(img_lst,folds)]\n",
    "X = []\n",
    "for s in img_lst:\n",
    "    X.append(cv2.resize(cv2.imread(s, cv2.IMREAD_COLOR),(150,150),interpolation=cv2.INTER_CUBIC))\n",
    "#X = [cv2.imshow('img',s) for s in X]\n",
    "\n",
    "y = np.array(train.classID.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  4   0   0]\n",
      "  [  4   0   0]\n",
      "  [  4   0   0]\n",
      "  ...\n",
      "  [  4   0   0]\n",
      "  [  4   0   0]\n",
      "  [  4   0   0]]\n",
      "\n",
      " [[ 16   5   6]\n",
      "  [ 16   5   6]\n",
      "  [ 16   5   6]\n",
      "  ...\n",
      "  [  4   0   0]\n",
      "  [  4   0   0]\n",
      "  [  4   0   0]]\n",
      "\n",
      " [[ 52  11  19]\n",
      "  [ 52  11  19]\n",
      "  [ 52  11  19]\n",
      "  ...\n",
      "  [  4   0   0]\n",
      "  [  4   0   0]\n",
      "  [  4   0   0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[119  19  68]\n",
      "  [119  19  68]\n",
      "  [119  19  68]\n",
      "  ...\n",
      "  [126  22  82]\n",
      "  [126  22  82]\n",
      "  [126  22  82]]\n",
      "\n",
      " [[112  14  60]\n",
      "  [112  14  60]\n",
      "  [112  14  60]\n",
      "  ...\n",
      "  [ 94  18  45]\n",
      "  [ 94  18  45]\n",
      "  [ 94  18  45]]\n",
      "\n",
      " [[ 89  16  41]\n",
      "  [ 89  16  41]\n",
      "  [ 89  16  41]\n",
      "  ...\n",
      "  [ 59  13  25]\n",
      "  [ 59  13  25]\n",
      "  [ 59  13  25]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3, 2, 2, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "print(X[0])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X /= 255.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7895, 150, 150, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# construct the image generator for data augmentation\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "aug = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "lb = LabelEncoder()\n",
    "y = np_utils.to_categorical(lb.fit_transform(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "#base = keras.applications.nasnet.NASNetLarge(input_shape=None, include_top=False, weights=None, input_tensor=None, pooling=None, classes=10)\n",
    "base = keras.applications.inception_resnet_v2.InceptionResNetV2(include_top=False, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(837, 150, 150, 3)\n"
     ]
    }
   ],
   "source": [
    "test_lst = np.array(test.slice_file_name.tolist())\n",
    "folds = np.array(test.fold)\n",
    "test_lst = [loc +'\\\\'+'fold'+str(fold)+'\\\\'+ s + '.jpg' for s,fold in zip(test_lst,folds)]\n",
    "#print(test_lst)\n",
    "test_X = []\n",
    "for s in test_lst:\n",
    "    test_X.append(cv2.resize(cv2.imread(s, cv2.IMREAD_COLOR),(150,150),interpolation=cv2.INTER_CUBIC))\n",
    "#X = [cv2.imshow('img',s) for s in X]\n",
    "test_X = np.array(test_X)\n",
    "test_X = test_X.astype('float')\n",
    "test_X /= 255.0 \n",
    "test_y = np.array(test.classID.tolist())\n",
    "test_y = np_utils.to_categorical(lb.fit_transform(test_y))\n",
    "print(test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "164/164 [==============================] - 364s 2s/step - loss: 0.7955 - categorical_accuracy: 0.7293 - top_k_categorical_accuracy: 0.9582\n",
      "Epoch 2/25\n",
      "164/164 [==============================] - 125s 761ms/step - loss: 0.2083 - categorical_accuracy: 0.9318 - top_k_categorical_accuracy: 0.9983\n",
      "Epoch 3/25\n",
      "164/164 [==============================] - 125s 762ms/step - loss: 0.0801 - categorical_accuracy: 0.9755 - top_k_categorical_accuracy: 0.9996\n",
      "Epoch 4/25\n",
      "164/164 [==============================] - 126s 770ms/step - loss: 0.0475 - categorical_accuracy: 0.9845 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 5/25\n",
      "164/164 [==============================] - 125s 762ms/step - loss: 0.0535 - categorical_accuracy: 0.9835 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 6/25\n",
      "164/164 [==============================] - 125s 762ms/step - loss: 0.0323 - categorical_accuracy: 0.9895 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 7/25\n",
      "164/164 [==============================] - 122s 746ms/step - loss: 0.0186 - categorical_accuracy: 0.9952 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 8/25\n",
      "164/164 [==============================] - 122s 746ms/step - loss: 0.0148 - categorical_accuracy: 0.9959 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 9/25\n",
      "164/164 [==============================] - 122s 747ms/step - loss: 0.0124 - categorical_accuracy: 0.9961 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 10/25\n",
      "164/164 [==============================] - 124s 754ms/step - loss: 0.0071 - categorical_accuracy: 0.9978 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 11/25\n",
      "164/164 [==============================] - 123s 753ms/step - loss: 0.0067 - categorical_accuracy: 0.9981 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 12/25\n",
      "164/164 [==============================] - 124s 753ms/step - loss: 0.0066 - categorical_accuracy: 0.9985 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 13/25\n",
      "164/164 [==============================] - 124s 753ms/step - loss: 0.0146 - categorical_accuracy: 0.9950 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 14/25\n",
      "164/164 [==============================] - 124s 754ms/step - loss: 0.0068 - categorical_accuracy: 0.9977 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 15/25\n",
      "164/164 [==============================] - 124s 753ms/step - loss: 0.0090 - categorical_accuracy: 0.9980 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 16/25\n",
      "164/164 [==============================] - 124s 753ms/step - loss: 0.0060 - categorical_accuracy: 0.9983 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 17/25\n",
      "164/164 [==============================] - 124s 754ms/step - loss: 0.0061 - categorical_accuracy: 0.9977 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 18/25\n",
      "164/164 [==============================] - 124s 754ms/step - loss: 0.0026 - categorical_accuracy: 0.9991 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 19/25\n",
      "164/164 [==============================] - 124s 753ms/step - loss: 0.0119 - categorical_accuracy: 0.9964 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 20/25\n",
      "164/164 [==============================] - 124s 753ms/step - loss: 0.0087 - categorical_accuracy: 0.9976 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 21/25\n",
      "164/164 [==============================] - 124s 753ms/step - loss: 0.0035 - categorical_accuracy: 0.9995 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 22/25\n",
      "164/164 [==============================] - 124s 756ms/step - loss: 0.0028 - categorical_accuracy: 0.9992 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 23/25\n",
      "164/164 [==============================] - 124s 759ms/step - loss: 0.0026 - categorical_accuracy: 0.9991 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 24/25\n",
      "164/164 [==============================] - 124s 756ms/step - loss: 0.0035 - categorical_accuracy: 0.9991 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 25/25\n",
      "164/164 [==============================] - 124s 757ms/step - loss: 0.0037 - categorical_accuracy: 0.9991 - top_k_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2861d443ef0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "BS = 48\n",
    "EPOCHS = 25\n",
    "x = base.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "model = Model(inputs=base.input, outputs=predictions)\n",
    "for layer in base.layers:\n",
    "    layer.trainable = True\n",
    "model.compile(optimizer=SGD(lr=0.01, momentum=0.9), loss='categorical_crossentropy', metrics=['categorical_accuracy','top_k_categorical_accuracy'])\n",
    "model.fit_generator(aug.flow(X, y, batch_size=BS),\n",
    "\tsteps_per_epoch = len(X) // BS,\n",
    "\tepochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(r'D:\\Projects\\urban_sound\\model\\model_10.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "837/837 [==============================] - 25s 29ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.071379462127399, 0.8243727598566308, 0.994026284348865]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=test_X, y=test_y, batch_size=None, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
